When I was in graduate school in Ann Arbor, I had a friend who was deeply involved with the environmentalist movement.

He purchased his food from local farmers’ markets, and he commuted by bike instead of by car to reduce his carbon footprint, and he maintained a horrid compost bin that will probably be the origin of the next flu ppandemic.

One day, he told me that he was going to visit a farm for a week.

I asked him why, and he said that he wanted to “get closer to the land,” a phrase that you can only say with a straight face if you’re narrating a documentary about ancient South American tribes.

I told my friend that the land didn’t want to get closer to him, and if he really looked at the land, he’d see that it was not composed of delicious organic trail mix, but famine and vultures and backbreaking labor involving wheelbarrows and generally un­acceptable quantities of insects.

He responded with an extended lecture about eco-responsibility, a lecture that I immediately forgot because I realized that my naïve friend was going to die on that farm.

So, I told my friend that he shouldn’t be afraid to end his trip early if he wasn’t having a good time.

He smiled at me, the way that people in slasher movies smile before they get chopped up, and he left for the farm.

Precisely 37 hours later, he called me on the phone.

I asked him how everything was going, and he made a haunting, elegiac noise, like a foghorn calling out for its mate.

I asked him to describe his first day, and he said that his entire existence revolved around bleating things: bleating goats that wanted to be fed, and bleating crows that wanted to steal the food that he gave the bleating goats, and bleating farm machines that were composed of spinning metal blades and had no discernable purpose besides enrolling you in the “Hook Hand of the Month” club.

I asked my friend when he was coming back home, and he said that he was calling me from the Ann Arbor train station; he had already returned.

And then he let out that foghorn noise, that awful, lingering sound, and I thought, MAYBE THAT’S THE FIRST SYMPTOM OF COMPOST BIN FLU.

Computer scientists often look at Web pages in the same way that my friend looked at farms.

People think that Web browsers are elegant computation platforms, and Web pages are light, fluffy things that you can edit in Notepad as you trade ironic comments with your friends in the coffee shop.

Nothing could be further from the truth.

A modern Web page is a catastrophe.

It’s like a scene from one of those apocalyptic medieval paintings that depicts what would happen if Galactus arrived: people are tumbling into fiery crevasses and lamenting various lamentable things and hanging from playground equipment that would not pass OSHA safety checks.

This kind of stuff is exactly what you’ll see if you look at the HTML,CSS, and JavaScript in a modern Web page.

Of course, no human can truly “look” at this content, because a Web page is now like V’Ger from the first “Star Trek” movie, a piece of technology that we once understood but can no longer fathom, a thrashing leviathan of code and markup written by people so untrustworthy that they’re not even third parties, they’re fifth parties who weren’t even INVITED to the party, but who showed up anyways because the hippies got it right and free love or whatever.

I’m pretty sure that the Web browser is one of the “dens of iniquity” that I keep hearing about on Fox News; I would verify this using a Web search, but a Web search would require me to use a browser, AND THIS IS EXACTLY WHAT BICOASTAL LIBERAL ELITES WANT ME TO DO.

Describing why the Web is horrible is like describing why it’s horrible to drown in an ocean composed of pufferfish that are pregnant with tiny Freddy Kruegers—each detail is horrendous in isolation, but the aggregate sum is delightfully arranged into a hate flower that blooms all year.

For example, the World Wide Web Consortium (W3C) provides “official” ­specifications for many client-side Web technologies.

Unfortunately, these specifications are binding upon browser vendors in the same way that you can ask a Gila monster to meet you at the airport, but that gila monster may, in fact, have better things to do.

Each W3C document is filled with alienating sentences that largely consist of hyperlinks to different hyperlinks.

For instance, if you’re a browser vendor, and you want to add support for HTML selectors, you should remember that, during the third step of parsing the selector string, “If ­­ result is invalid ([SELECT], section 12), raise a SYNTAX_ERR exception ([DOM-LEVEL-3-CORE] , section 1.4) and abort this algorithm.”

 Such bodice-ripping legalese is definitely exciting for people who yearn for the dullness of the Cheerios ingredient list combined with the multi-layered bureaucracy of the Soviet Union.

Indeed, you could imagine a world in which browser vendors hire legions of Talmudic scholars to understand why, precisely, SYNTAX_ERR is orange and not mauve, and how, exactly, this orangeness relates to the parenthetic purpleness of ([DOM-LEVEL-3-CORE] ).

You could also imagine a world in which browser vendors do not do this, and instead implement 53% of each spec and then hope that no Web page tries to use HTML selectors and then the geolocation interface and then a <canvas> tag, because that sequence of events will unleash the Antichrist and/or a rendered Web page that looks like one of those Picasso paintings that you pretend to understand, but which everyone wants to throw into an ocean because nobody wants to look at a painting of a blue man who is composed of isosceles triangles and has a guitar emerging from his forehead for no reason at all.

Given the unbearable proliferation of Web standards, and the comically ill-expressed semantics of those standards, browser vendors should just give up and tell society to stop asking for such ridiculous things.

However, this opinion is unpopular, because nobody will watch your TED talk if your sense of optimism is grounded in reality.

I frequently try to explain to my friends why they should abandon Web pages and exchange information using sunlight reflected from mirrors, or the enthusiastic waving of colored flags.

My friends inevitably respond with a spiritually vacant affirmation like, “People invented flying machines, so we can certainly make a good browser!

” Unfortunately, defining success for a flying machine is easy (“I’M ME BUT I’M A BIRD”), whereas defining success for a Web browser involves Cascading Style Sheets, a technology which intrinsically dooms any project to epic failure.

For the uninitiated, Cascading Style Sheets are a cryptic language developed by the Freemasons to obscure the visual nature of reality and encourage people to depict things using ASCII art.

Ostensibly, CSS files allow you to separate the definition of your content from the definition of how that content looks—using CSS, you can specify the layout for your HTML tags, as well as the fonts and the color schemes used by those tags.

Sadly, the relationship between CSS and HTML is the same relationship that links the instructions for building your IKEA bed, and the unassembled, spiteful wooden planks that purportedly contain latent bed structures.

CSS is not so much a description of what your final page will look like, but rather a loose, high-level overview of what could happen to your page, depending on the weather, the stock market, and how long it’s been since you last spoke to your mother.

Like a naïve Dungeon Master untouched by the sorrow of adulthood, you create imaginative CSS classes for your <div> tags and your <span> tags, assigning them strengths and weaknesses, and defining the roles that they will play in the larger, uplifting narrative of your HTML.

Everything is assembled in its proper place; you load your page in a browser and prepare yourself for a ­glorious victory.

However, you quickly discover that your elf tag is overweight.

THE ELF CAN NEVER BE OVERWEIGHT.

Even worse, your barbarian tag does not have an oversized hammer or axe.

Without an oversized hammer or axe, YOUR BARBARIAN IS JUST AN ILLITERATE STEROID USER.

And then you look at your wizard tag, and you see that he’s not an old white man with a flowing beard, but a young black man from Brooklyn.

FOR COMPLEX REASONS THAT ARE ROOTED IN EUROPEAN COLONIAL NARRATIVES, YOUR WIZARD MUST BE AN OLD WHITE MAN WITH A FLOWING BEARD, NOT A BLACK MAN WITH HIPSTER SHOES AND A FANTASTIC VINYL COLLECTION.

Such are the disasters that CSS will wroughth upon thee.

Or wrought *at* thee.

To be honest, I don’t know how to conjugate or spell “wrought,” but my point is undoubtedly understood. Or CSS’ (no trailing “s”) wroughtiness.

MY NON-CASCADING STYLE MANUALS FIGHT FOR MY SOUL.

When you’re a Web developer, CSS is just one of your worries.

The aggregate stack of Web technologies is so fragile that developers just accept a world in which various parts of a Web page will fail at random times.

Apparently this is okay because e-commerce isn’t a serious thing, and if you really wanted a secure banking experience, you’d visit the bank in person like someone from the 1800s instead of accessing a banking Web site that is constantly (but silently) vomiting execution errors to the console log (a console log which the browser does not show by default, because if you knew about it, and you read its tales of woe, you’d abandon computer science and become a maker of fine wooden shoes).

In Figure 2, I provide an unaltered example of such a console log; the log was generated by a real Web page from a popular site.

One time, I tried to build a browser-agnostic debugging infrastructure.

I had a client-side JavaScript library that could traverse the JavaScript heap and display fun things about the page’s state.

My art history friends told me that console output is for Neanderthals, so I made an HTML GUI to display the diagnostic information.

The first version of the GUI used the browser’s default layout policies.

Much like Icarus, I dreamt of more, so I decided to make A Fancy LayoutTM.

I wrote CSS that specified whether my tags should have static positioning, or floating positioning, or relative zodiac-based positioning.

Here’s what I learned: Never specify whether your tags should be static or floating or zodiac-based.

As soon as a single tag is released from the automatic layout process, the browser will immediately go insane and stack random HTML tags along the z-axis, an axis which apparently is an option even if your monitor can only display two dimensions.

I eventually found a working CSS file inside a bottle that washed up on the beach, and I tweaked the file until it worked for my GUI.

Then I went home and cried big man tears that were filled with ninja stars and that turned into lions when they hit the ground.

The first log entry says that the browser executed a downloaded file as JavaScript, even though the MIME type of the file was text/html.

Here’s a life tip: when you’re confused about what something is, DON’T EXECUTE IT TO DISCOVER MORE CLUES.

This is like observing that your next-door neighbor is a creepy, bedraggled man with weird eyes, and then you start falling asleep on his doorstep using a chloroform rag as a pillow, just to make sure that he’s not going to tie you to a radiator and force you to paint tiny figurines. Here’s how your life story ends: YOU ARE A PAINTER OF TINY FIGURINES.

The second and third errors say that the page’s JavaScript used a variable name that is deprecated in strict mode, but acceptable in quirks mode.

How can I begin to explain this delicious confection of awfulness?

Listen: when a man and a woman fall in love, they want to demonstrate their love to each other.

So, they force browsers to support different types of runtime environments.

“Standards mode” refers to the unreliable browser APIs that are described by recent HTML and CSS specifications.

“Quirks mode” refers to the unreliable browser APIs that were defined by browsers from the Eisenhower administration.

Quirks mode was originally invented because many Web pages were made during the Eisenhower administration, and the computing industry wanted to preserve Web-based narratives about why “the rock and roll” is corrupting our youth.

Quirks mode then persisted because Web developers learned about quirks mode and used it as an excuse to not learn new skills.

But then some Web developers wanted to learn new skills, so standards mode was invented to allow these developers to make old mistakes in new ways.

There is also a third browser mode called “almost standards mode”; this mode is similar to standards mode, except that it renders images inside table cells using the quirks mode algorithm.

Yhey said that I could become anything, so I became the error log of a Web browser.

Now I own fifteen cats and I wonder where the parties are.

For reasons that have been eaten by a wildebeest, “almost standards mode” is also called “strict” mode, even though it is less strict than standards mode.

For reasons so horrendous that the wildebeest would not eat them, there is no completely reliable way to make all browsers load your page using the same compatibility mode.

Thus, even if your page recites the recommended incantations, the browser may still do what it wants to do, how it wants to do it.

And that’s where babies come from.

The fourth and seventh errors represent uncaught JavaScript exceptions.

In a rational universe, a single uncaught exception would terminate a program, and if a program continued to execute after throwing such an exception, we would know that Ragnarok is here and Odin is not happy.

In the browser world, ignoring uncaught exceptions is called “Wednesday, and all days not called ‘Wednesday.’”

The JavaScript event loop is quite impervious to conventional notions of software reliability, so if an event handler throws an exception, the event loop will literally pretend like nothing happened and keep running.

This ludicrous momentum continues even if, in the case of the seventh error, the Web page tries to call init() on an object that has no init() method.

You should feel uncomfortable that a Web page can disagree with itself about the existence of initialization routines, but the page is still allowed to do things with things.

Such a dramatic mismatch of expectations would be unacceptable in any other context.

You would be sad if you went to the hospital to have your appendix removed, and the surgeon opened you up, and she said, “I DIDN’T EXPECT YOUR LIVER TO HAVE GILLS,” and then she proceeded with her original surgical plan, despite the fact that you’re apparently a mer-person.

Being a mer-person should have non-ignorable ramifications in the material universe.

Similarly, if a Web page thinks than an object should be initialized, but the object has no initialization method, the browser shouldn’t laugh about it and then proceed under the assumption that the rest of the page is agnostic about whether its objects are composed of folly.

An interpretation of the remaining errors is left as an exercise to the reader.

Note that understanding the eighth error requires a Ouija board, the eye of a newt, and the whispering of a secret to a long-lost friend.

At this point, it should be intuitively obvious that different browsers may or may not produce the same error log for the same page.

In general, if a Web page has more than three bits of entropy, different browsers will generate extravagantly unique mappings between the Web developer’s intentions and the schizophrenic beast palette that browsers use to paint the world.

Thus, picking the “best browser” is like playing one of those horrid trust-building exercises where you decide which three of your five senses you would prefer to lose, and then your coworkers berate you for making different tradeoffs than they made, even though there is no partial ordering that relates scuba diving accidents in which you lose your ears and eyes, and industrial accidents in which you lose your nose and tongue.

All options are bad options; it’s a world of lateral moves.

Indeed, trying to pick the best browser is like trying to decide which of your worthless children should inherit the family business.

Little Oliver refuses to accept society’s notion of what an event handling loop should do, so whenever the user presses a key on the keyboard, Oliver does not fire one keyPress event, but instead three keyDown events, a keyUp event, and the deleted saxophone solo from Mozart’s eighth symphony.

Dearest Fiona, an unrepentant workaholic, designed her browser so that when you “close” it, the GUI goes away, but the underlying process lingers in the background, silent and angry, slowly consuming entries in kernel tables and making it impossible to restart the browser without receiving the error message “Somewhere in this world, another copy of the browser is running; find Carmen Sandiego and she will reveal the truth.

Beloved Christopher, in an attempt to make his browser fast and lightweight, decided to replace his Flash plugin with code that prints “Shockwave has crashed” and then immediately dereferences a NULL pointer; this ensures that most attempts to watch a video will end with you wishing for the simpler audiovisual pleasures of a woodcut or cave painting.

And poor IE6, voted “Least Likely to Succeed Because IE6 Is Not a Proper Christian Name,” manages to stumble through the world while surviving more assassination attempts than Fidel Castro.

Each browser is reckless and fanciful in its own way, but all browsers share a love of epic paging to disk.

Not an infrequent showering of petite I/Os that are aligned on the allocation boundaries of the file system—I mean adversarial thunder­ snows of reads and writes, a primordial deluge that makes you gather your kinfolk and think about which things you need two of, and what the consequences would be if you didn’t bring fire ants, because fire ants ruin summers.

Browsers don’t require a specific reason to thrash the disk; instead, paging is a way of life for browsers, a leisure activity that is fulfilling in and of itself.

If you’re not a computer scientist or a tinkerer, you just accept the fact that going to CNN.com will cause the green blinky light with the cylinder icon to stay green and not blinky.

However, if you know how computers work, the incessant paging drives you mad.

It turns you into Torquemada, a wretched figure consumed by the fear that your ideological system is an elaborate lie designed to hide the excessive disk seeks of shadowy overlords.

You launch your task manager, and you discover that your browser has launched 67 different processes, all of which are named “browser.exe,” and all of which are launching desperate volleys of I/Os to cryptic parts of the file system like “\roaming\pots\pans\cache\4$$Dtub.partial”, where “\4$$” is an exotic escape sequence that resolves to the Latvian double umlaut.

You do an Internet search for potential solutions, and you’re confronted with a series of contradictory, ill-founded opinions: your browser has a virus; your virus has a virus; you should be using Emacs; you should be using vi, and this is why your marriage is loveless.

Of course, the most popular advice for solving any browser problem is to clear your browser cache.

It is definitely true that emptying the cache will sometimes help, in the same sense that if you’re poor, kicking a tree will sometimes lead to a ­hilarious series of events that conclude with you finding a big bag of money on the ground with a note that says, “Spend it all! XOXO, Life.”

Unfortunately, kicking a tree does not typically lead to riches, so your faith-based act of tree assault really just makes you a savage, tree-kicking monster who will be vilified by children and emotionally sensitive adults.

Similarly, your arbitrary clearing of the browser cache, however well-intentioned, is just a topical anesthetic to briefly dull the pain of existence.

Clearing the cache to fix a Web browser is like when your dad was driving you to kindergarten, and the car started to smoke, and he tried to fix the car by banging on the hood three times and then asking you if you could still smell the carbon monoxide, and you said, “Yeah, it’s better,” because you didn’t want to expose your dad as a fraud, and then both of you rode to school in silence as you struggled to remain conscious.

So, yes, it would be great if fixing your browser involved actions that were not semantically equivalent to voodoo.

But, on the bright side, things could always be worse.

For example, it would definitely be horrible if your browser’s scripting language combined the prototype-based inheritance of Self, a quasi-functional aspect borrowed from LISP, a structured syntax adapted from C, and an aggressively asynchronous I/O model that requires elaborate callback chains that span multiple generations of hard-working Americans.

OH NO I’VE JUST DESCRIBED JAVASCRIPT.

What an unpleasant turn of events!

People were begging for a combination of Self, LISP, and C in the same way that the denizens of Middle Earth were begging Saruman to breed Orcs and men to make Uruk-hai.

Orcs and men were doing a fine job of struggling in their separate communities—creating a new race with the drawbacks of both is not a good way to win popularity contests.

But despite its faults, JavaScript has become widespread.

Discovering why this happened is similar to understanding the causes for World War I—everyone agrees on the top five reasons, but everyone ranks those causes differently.

The basic story is that, in the ’90s, when JavaScript and Java were competing for client-side supremacy, Java applets were horrendously slow and lacked a story for interacting with HTML; in contrast, JavaScript was only semi-horrendously slow, and it had a bad (but extant) story for interacting with HTML.

JavaScript is dynamically typed, and its aggressive type coercion rules were apparently designed by Monty Python.

For example, 12 == “12” because the string is coerced into a num-
ber. This is a bit silly, but it kind of makes sense.

Now consider the fact that null == undefined. That is completely janky; areference that points to null is not undefined—IT IS DEFINED
AS POINTING TO THE NULL VALUE.

And now that you’re warmed up, look at this: “\r\n\t” == false.

 Here’s why: the browser detects that the two operands have different types, so it converts false to 0 and retries the comparison. The operands
still have different types (string and number), so the browser
coerces “\r\n\t” into the number 0 , because somehow, a
non-zero number of characters is equal to 0 . Voila — 0 equals
0 ! AWESOME.

That explanation was like the plot to Inception, but the implanted idea was “the correctness of your program has been coerced to false .”

Hello, kind stranger—let me keep you warm during this cold
winter night!

Did you know that JavaScript defines a special NaN (“not a number”) value? This value is what you get when you do foolish things like parseInt(“BatmanIsNotAnInteger”).

In other words, NaN is a value that is not indicative of a number. However, typeof(NaN) returns... “number.”

 A more obvious return value would be “HAIL BEELZEBUB, LORD OF DARKNESS,” but I digress.

By the way, NaN != NaN , so Aristotle was wrong about that whole “Law of Identity” thing.


DECAPITATE ME AND BURN MY WRITHING BODY WITH FIRE.

I obviously get what I deserve if my JavaScript library redefines native prototypes in a way that breaks my own code.

However, a single frame in a Web page contains multiple JavaScript libraries from multiple origins, so who knows what kinds of horrendous prototype manipulations those heathen libraries did before my library even got to run.

This is just one of the reasons why the phrase “JavaScript security” causes Bibles to burst into flames.

Also, JavaScript defines two identity operators (=== and ! == operators) which don’t perform the type coercion that the standard equality operators do; however, NaN ! == NaN.

So, basically, don’t use numbers in JavaScript, and if you absolutely have to use numbers, implement a software-level ALU. It’s slow, but it’s the only way to be sure. Actually, you still can’t be sure.

Unlike C++, which uses statically declared class interfaces, JavaScript uses prototype-based inheritance.

A prototype is a dynamically defined object which acts as an exemplar for “instances” of that object.

Much like C, JavaScript uses semicolons to terminate many kinds of statements.

However, in JavaScript, if you forget a semicolon, the JavaScript parser can automatically insert semicolons where it thinks that semicolons might ought to possibly maybe go.

This sounds really helpful until you realize that semicolons have semantic meaning.

You can’t just scatter them around like you’re the Johnny Appleseed of punctuation.

Automatically inserting semicolons into source code is like mishearing someone over a poor cell-phone connection, and then assuming that each of the dropped words should be replaced with the phrase “your mom.” This is a great way to create excitement in your interpersonal relationships, but it is not a good way to parse code.

Some JavaScript libraries intentionally begin with an initial semicolon, to ensure that if the library is appended to another one (e.g., to save HTTP roundtrips during download), the JavaScript parser will not try to merge the last statement of the first library and the first statement of the second library into some kind of semicolon-riven statement party.

Such an initial semicolon is called a “defensive semicolon.” That is the saddest programming concept that I’ve ever heard, and I am fluent in C++.

I could go on and on about the reasons why JavaScript is a cancer upon the world.

I know that there are people who like JavaScript, and I hope that these people find the mental health services that they so desperately need.

I don’t know all of the answers in life, but I do know all of the things which aren’t the answers, and JavaScript falls into the same category as Scientology, homeopathic medicine, and making dogs wear tiny sweaters due to a misplaced belief that this is what dogs would do if they had access to looms and opposable thumbs.

In summary, Web browsers are like quantum physics: they offer probabilistic guarantees at best, and anyone who claims to fully understand them is a liar.

At this stage in human development, there are big problems to solve: climate change, heart disease, the poor financial situation of Nigerian princes who want to contact you directly.

With all of these problems unsolved, Web browsing is a terrible way to spend our time; the last thing that we should do is run unstable hobbyist operating systems that download strange JavaScript files from people we don’t know.

Instead, we should exchange information using fixed-length ASCII messages written in a statically verifiable subset of Latin, with images represented as mathematical combinations of line segments, arcs, and other timeless shapes described by dead philosophers who believed that minotaurs were real but incapable of escaping mazes.

That is the kind of clear thinking that will help us defeat the space Egyptians that emerge from the StarGates. Or whatever.

I’m an American and I don’t really understand history, but I strongly believe that Greeks spoke Latin to defeat intergalactic Egyptians.

#TeachTheControversy!

Anyways, my point is that browsers are too complex to be trusted.

Unfortunately, youth is always wasted on the young, and the current generation of software developers is convinced that browsers need more features, not fewer.

So, we are encouraged to celebrate the fact that browsers turn our computers into little Star Wars cantinas where everyone is welcome and you can drink a blue drink if you want to drink a blue drink and if something bad happens, then maybe a Jedi will save you, and if not, HEY IT’S A STAR WARS CANTINA YESSSSS.

Space cantinas are fun, but they’re just a fantasy; they’re just a series of outlandish details stitched together to amuse and entertain.

You have to open your eyes and see that in the real, non-hyperbolic world that you actually inhabit, your browser will frequently stop playing a video and then display flashing epilepsy pixels while making the sound that TVs make in Japanese horror movies before a pasty salamander child steps out of the screen and voids your warranty. That’s a thing which could actually happen, and we should wash it all away.

Whenever I go to a conference and I discover that there will be
a presentation about Byzantine fault tolerance, I always feel
an immediate, unshakable sense of sadness, kind of like when
you realize that bad things can happen to good people, or that Keanu Reeves will almost certainly make more money than you over arbitrary time scales.

Watching a presentation on Byzantine fault tolerance is similar to watching a foreign film from a depressing nation that used to be controlled by the Soviets—the only difference is that computers and networks are constantly failing instead of young Kapruskin being unable to reunite with the girl he fell in love with while he was working in a coal mine beneath an orphanage that was atop a prison that was inside the abstract concept of World War II. 

“How can you make a reliable computer service?” the presenter will ask in an innocent voice before continuing,

“It may be difficult if you can’t trust anything and the entire concept of happiness is a lie designed by unseen overlords of endless deceptive power.”

Making distributed systems reliable is inherently impossible; we cling to Byzantine fault tolerance like Charlton Heston clings to his guns, hoping that a series of complex software protocols will somehow protect us from the oncoming storm of furious apes who have somehow learned how to wear pants and maliciously tamper with our network packets.

The caption should really say, “One day, a computer
wanted to issue a command to an online service. This simple
dream resulted in the generation of 16 gajillion messages.”
“These messages will do things that only Cthulu understands; we are at peace with his dreadful mysteries, and we hope that you feel the same way.”

“Note that, with careful optimization, only 14 gajillion messages are necessary. This is still too many messages; however, if the system sends fewer than 14 gajillion messages, it will be vulnerable to accusations that it only handles reasonable failure cases, and not the demented ones that previous researchers spitefully introduced in earlier papers in a desperate attempt to distinguish themselves from even more prior (yet similarly demented) work.”

“As always, we are nailed to a cross of our own construction.”

In a paper about Byzantine fault tolerance, the related work section will frequently say, “Compare the protocol diagram of our system to that of the best prior work. Our protocol is clearly better.”

Trying to determine which one of these hateful diagrams is better is like gazing at two unfathomable seaweed bundles that washed up on the beach and trying to determine which one is marginally less alienating.

Listen, regardless of which Byzantine fault tolerance protocol you pick, Twitter will still have fewer than two nines of availability.

As it turns out, Ted the Poorly Paid Datacenter Operator will not send 15 cryptographically signed messages before he accidentally spills coffee on the air conditioning unit and then overwrites your tape backups with bootleg recordings of Nickelback.

Ted will just do these things and then go home, because that’s what Ted does.

His extensive home collection of “Thundercats” cartoons will not watch itself. Ted is needed, and Ted will heed the call of duty.

Every paper on Byzantine fault tolerance introduces a new kind of data consistency. This new type of consistency will have an ostensibly straightforward yet practically inscrutable name like “leap year triple-writer dirty-mirror asynchronous semi-consistency.”

In Section 3.2 (“An Intuitive Overview”), the authors will provide some plainspoken, spiritually appealing arguments about why their system prevents triple-conflicted write hazards in the presence of malicious servers and unexpected outbreaks of the bubonic plague.

“Intuitively, a malicious server cannot lie to a client because each message is an encrypted, nested, signed, mutually-attested log entry with pointers to other encrypted and nested (but not signed) log entries.”

Interestingly, these kinds of intuitive arguments are not intuitive.

A successful intuitive explanation must invoke experiences that I have in real life. I have never had a real-life experience that resembled a Byzantine fault tolerant protocol.

JAMES: I announce my desire to go to lunch.

JAMES: OH NO. LET ME TELL YOU AGAIN THAT I WANT TO GO TO LUNCH.

In conclusion, I think that humanity should stop publishing papers about Byzantine fault tolerance.

I do not blame my fellow researchers for trying to publish in this area, in the same limited sense that I do not blame crackheads for wanting to acquire and then consume cocaine.

The desire to make systems more reliable is a powerful one; unfortunately, this addiction, if left unchecked, will inescapably lead to madness and/or tech reports that contain 167 pages of diagrams and proofs.

Even if we break the will of the machines with formalism and cryptography, we will never be able to put Ted inside of an encrypted, nested log, and while the datacenter burns and we frantically call Ted’s pager, we will realize that Ted has already left for the cafeteria

As a highly trained academic researcher, I spend a lot of time trying to advance the frontiers of human knowledge.

However, as someone who was born in the South, I secretly believe that true progress is a fantasy, and that I need to prepare for the end times, and for the chickens coming home to roost, and fast zombies, and slow zombies, and the polite zombies who say “sir” and “ma’am” but then try to eat your brain to acquire your skills.

When the revolution comes, I need to be prepared; thus, in the quiet moments, when I’m not producing incredible scientific breakthroughs, I think about what I’ll do when the weather forecast inevitably becomes RIVERS OF BLOOD ALL DAY EVERY DAY.

The main thing that I ponder is who will be in my gang, because the likelihood of post-apocalyptic survival is directly related to the size and quality of your rag-tag group of associates.

There are some obvious people who I’ll need to recruit: a locksmith (to open doors); a demolitions expert (for when the locksmith has run out of ideas); and a person who can procure, train, and then throw snakes at my enemies (because, in a world without hope, snake throwing is a reasonable way to resolve disputes).

All of these people will play a role in my ultimate success as a dystopian warlord philosopher.

However, the most important person in my gang will be a systems programmer.

A person who can debug a device driver or a distributed system is a person who can be trusted in a Hobbesian nightmare of breathtaking scope; a systems programmer has seen the terrors of the world and understood the intrinsic horror of existence.

The systems programmer has written drivers for buggy devices whose firmware was implemented by a drunken child or a sober goldfish.

The systems programmer has traced a network problem across eight machines, three time zones, and a brief diversion into Amish country, where the problem was transmitted in the front left hoof of a mule named Deliverance.

The systems programmer has read the kernel source, to better understand the deep ways of the universe, and the systems programmer has seen the comment in the scheduler that says “DOES THIS WORK LOL,” and the systems programmer has wept instead of LOLed, and the systems programmer has submitted a kernel patch to restore balance to The Force and fix the priority inversion that was causing MySQL to hang.

A systems programmer will know what to do when society breaks down, because the systems programmer already lives in a world without law.


Listen: I’m not saying that other kinds of computer people are useless.

I believe (but cannot prove) that PHP developers have souls.

I think it’s great that database people keep trying to improve select-from-where, even though the only queries that cannot be expressed using select-from-where are inappropriate limericks from “The Canterbury Tales.”

In some way that I don’t yet understand, I’m glad that theorists are investigating the equivalence between five-dimensional Turing machines and Edward Scissorhands.

In most situations, GUI designers should not be forced to fight each other with tridents and nets as I yell “THERE ARE NO MODAL DIALOGS IN SPARTA.”

I am like the Statue of Liberty: I accept everyone, even the wretched and the huddled and people who enjoy Haskell.

But when things get tough, I need mission-critical people; I need a person who can wear night-vision goggles and descend from a helicopter on ropes and do classified things to protect my freedom while country music plays in the background.

I can realistically give a kernel hacker a nickname like “Diamondback” or “Zeus Hammer.”

In contrast, no one has ever said, “These semi-transparent icons are really semi-transparent! IS THIS THE WORK OF ZEUS HAMMER?”

I picked that last example at random.

You must believe me when I say that I have the utmost respect for HCI people.

However, when HCI people debug their code, it’s like an art show or a meeting of the United Nations. There are tea breaks and witticisms exchanged in French; wearing a non-functional scarf is optional, but encouraged.

When HCI code doesn’t work, the problem can be resolved using grand theories that relate form and perception to your deeply personal feelings about ovals.

There will be rich debates about the socioeconomic implications of Helvetica Light, and at some point, you will have to decide whether serifs are daring statements of modernity, or tools of hegemonic oppression that implicitly support feudalism and illiteracy.

Is pinching-and-dragging less elegant than circling-and-lightly-caressing?

These urgent mysteries will not solve themselves.

And yet, after a long day of debugging HCI code, there is always hope, and there is no true anger; even if you fear that your drop-down list should be a radio button, the drop-down list will suffice until tomorrow, when the sun will rise, glorious and vibrant, and inspire you to combine scroll bars and left-clicking in poignant ways that you will commemorate in a sonnet when you return from your local farmer’s market.

This is not the world of the systems hacker.

When you debug a distributed system or an OS kernel, you do it Texas-style.

You gather some mean, stoic people, people who have seen things die, and you get some primitive tools, like a compass and a rucksack and a stick that’s pointed on one end, and you walk into the wilderness and you look for trouble, possibly while using chewing tobacco.

As a systems hacker, you must be prepared to do savage things, unspeakable things, to kill runaway threads with your bare hands, to write directly to network ports using telnet and an old copy of an RFC that you found in the Vatican.

When you debug systems code, there are no high-level debates about font choices and the best kind of turquoise, because this is the Old Testament, an angry and monochromatic world, and it doesn’t matter whether your Arial is Bold or Condensed when people are covered in boils and pestilence and Egyptian pharaoh oppression.

HCI people discover bugs by receiving a concerned email from their therapist.

Systems people discover bugs by waking up and discovering that their first-born children are missing and “ETIMEDOUT ” has been written in blood on the wall.

What is despair? I have known it—hear my song.

Despair is when you’re debugging a kernel driver and you look at a memory dump and you see that a pointer has a value of 7.

THERE IS NO HARDWARE ARCHITECTURE THAT IS ALIGNED ON 7.

Furthermore, 7 IS TOO SMALL AND ONLY EVIL CODE WOULD TRY TO ACCESS SMALL NUMBER MEMORY.

Misaligned, small-number memory accesses have stolen decades from my life.

The only things worse than misaligned, small-number memory accesses are accesses with aligned buffer pointers, but impossibly large buffer lengths.

Nothing ruins a Friday at 5 P.M. faster than taking one last pass through the log file and discovering a word-aligned buffer address, but a buffer length of NUMBER OF ELECTRONS IN THE UNIVERSE.

This is a sorrow that lingers, because a 2^893 byte read is the only thing that both Republicans and Democrats agree is wrong.

It’s like, maybe Medicare is a good idea, maybe not, but there’s no way to justify reading everything that ever existed a jillion times into a mega-jillion sized array.

This constant war on happiness is what non-systems people do not understand about the systems world.

I mean, when a machine learning algorithm mistakenly identifies a cat as an elephant, this is actually hilarious.

You can print a picture of a cat wearing an elephant costume and add an ironic caption that will entertain people who have middling intellects, and you can hand out copies of the photo at work and rejoice in the fact that everything is still fundamentally okay.

There is nothing funny to print when you have a misaligned memory access, because your machine is dead and there are no printers in the spirit world.

An impossibly large buffer error is even worse, because these errors often linger in the background, quietly overwriting your state with evil; if a misaligned memory access is like a criminal burning down your house in a fail-stop manner, an impossibly large buffer error is like a criminal who breaks into your house, sprinkles sand atop random bedsheets and toothbrushes, and then waits for you to slowly discover that your world has been tainted by madness

Indeed, the common discovery mode for an impossibly large buffer error is that your program seems to be working fine, and then it tries to display a string that should say “Hello world,” but instead it prints “#a[5]:3!” or another syntactically correct Perl script, and you’re like WHAT THE HOW THE, and then you realize that your prodigal memory accesses have been stomping around the heap like the Incredible Hulk when asked to write an essay entitled “Smashing Considered Harmful.”

You might ask, “Why would someone write code in a grotesque language that exposes raw memory addresses? Why not use a modern language with garbage collection and functional programming and free massages after lunch?”

Here’s the answer: Pointers are real. They’re what the hardware understands. Somebody has to deal with them.

You can’t just place a LISP book on top of an x86 chip and hope that the hardware learns about lambda calculus by osmosis.

Denying the existence of pointers is like living in ancient Greece and denying the existence of Krackens and then being confused about why none of your ships ever make it to Morocco, or Ur-Morocco, or whatever Morocco was called back then.

Pointers are like Krackens—real, living things that must be dealt with so that polite society can exist.

Make no mistake, I don’t want to write systems software in a language like C++.

Similar to the Necronomicon, a C++ source code file is a wicked, obscure document that’s filled with cryptic incantations and forbidden knowledge.

When it’s 3 A.M., and you’ve been debugging for 12 hours, and you encounter a virtual static friend protected volatile templated function pointer, you want to go into hibernation and awake as a werewolf and then find the people who wrote the C++ standard and bring ruin to the things that they love.

The C++ STL, with its dyslexia-inducing syntax blizzard of colons and angle brackets, guarantees that if you try to declare any reasonable data structure, your first seven attempts will result in compiler errors of Wagnerian fierceness:

Syntax error: unmatched thing in thing from std::nonstd::_ _map<_Cyrillic, _$$$dollars>const basic_string< epic_mystery,mongoose_traits &lt; char>, _ _default_alloc_<casual_Fridays = maybe>>

One time I tried to create a list<map<int>>, and my syntax errors caused the dead to walk among the living.

Such things are clearly unfortunate.

Thus, I fully support high-level languages in which pointers are hidden and types are strong and the declaration of data structures does not require you to solve a syntactical puzzle generated by a malevolent extraterrestrial species.

That being said, if you find yourself drinking a martini and writing programs in garbage-collected, object-oriented Esperanto, be aware that the only reason that the Esperanto runtime works is because there are systems people who have exchanged any hope of losing their virginity for the exciting opportunity to think about hex numbers and their relationships with the operating system, the hardware, and ancient blood rituals that Bjarne Stroustrup performed at Stonehenge.

Perhaps the worst thing about being a systems person is that other, non-systems people think that they understand the daily tragedies that compose your life.

For example, a few weeks ago, I was debugging a new network file system that my research group created.

The bug was inside a kernel-mode component, so my machines were crashing in spectacular and vindictive ways.

After a few days of manually rebooting servers, I had transformed into a shambling, broken man, kind of like a computer scientist version of Saddam Hussein when he was pulled from his bunker, all scraggly beard and dead eyes and florid, nonsensical ramblings about semi-imagined enemies.

As I paced the hallways, muttering Nixonian rants about my code, one of my colleagues from the HCI group asked me what my problem was.

I described the bug, which involved concurrent threads and corrupted state and asynchronous message delivery across multiple machines, and my coworker said, “Yeah, that sounds bad. Have you checked the log files for errors?”

I said, “Indeed, I would do that if I hadn’t broken every component that a logging system needs to log data. I have a network file system, and I have broken the network, and I have broken the file system, and my machines crash when I make eye contact with them.” 

“I HAVE NO TOOLS BECAUSE I’VE DESTROYED MY TOOLS WITH MY TOOLS.”

“My only logging option is to hire monks to transcribe the subjective experience of watching my machines die as I weep tears of blood.”

My coworker, in an earnest attempt to sympathize, recounted one of his personal debugging stories, a story that essentially involved an addition operation that had been mistakenly replaced with a multiplication operation.

I listened to this story, and I said, “Look, I get it. Multiplication is not addition. This has been known for years. However, multiplication and addition are at least related. Multiplication is like addition, but with more addition. Multiplication is a grown-up pterodactyl, and addition is a baby pterodactyl.”

“Thus, in your debugging story, your code is wayward, but it basically has the right idea.”

“In contrast, there is no family-friendly GRE analogy that relates what my code should do, and what it is actually doing.”

“I had the modest goal of translating a file read into a network operation, and now my machines have tuberculosis and orifice containment
issues.”

“Do you see the difference between our lives? When you asked a girl to the prom, you discovered that her father was a cop.”

“When I asked a girl to the prom, I DISCOVERED THAT HER FATHER WAS STALIN.”

In conclusion, I’m not saying that everyone should be a systems hacker. GUIs are useful. Spell-checkers are useful.

I’m glad that people are working on new kinds of bouncing icons because they believe that humanity has solved cancer and homelessness and now lives in a consequence-free world of immersive sprites.

That's exciting, and I wish I could join those people in the 27th century.

But I live here, and I live now, and in my neighborhood, people are dying in the streets.

It's like, French is a great idea, but nobody is going to invent French if they're constantly being attacked by bears.

Do you see? SYSTEMS HACKERS SOLVE THE BEAR MENACE.

Only through the constant vigilance of my people do you get the freedom o think about croissants and subtle puns involving the true father of Louis XIV.

So, if you see me wandering the halls, trying to explain synchronization bugs to confused monks, rest assured that every day, in every way, it gets a little better. For you, not me.

I'll always be furious at the number 7, but such is the hero's journey.
